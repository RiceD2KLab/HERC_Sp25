import os
import pandas as pd
import glob
import re

# Define Desktop Path & Load Files
desktop_path = os.path.expanduser("~/Desktop")
file_pattern = os.path.join(desktop_path, "master_sheet_20*.csv")

# Load all master_sheet CSVs from 2020 to 2023
csv_files = sorted(glob.glob(file_pattern))

if not csv_files:
    print("‚ùå No master_sheet files found on the desktop.")
    exit()

print(f"‚úÖ Found {len(csv_files)} master sheets on desktop.\n")

# Step 1: Identify Truly Common Columns (excluding 'year')
column_sets = []

for file in csv_files:
    df = pd.read_csv(file)
    
    # Standardize column names (lowercase & strip spaces)
    df.columns = df.columns.str.lower().str.strip()

    # Exclude 'year' column if it exists
    column_sets.append(set(df.columns) - {'year'})

# Find the common columns across all datasets
common_columns = set.intersection(*column_sets)

# Print the identified common columns
print("\n‚úÖ Final common columns across all master sheets (excluding 'year'):")
print(sorted(common_columns))

# Step 2: Merge All Sheets with a Full Outer Join on Common Columns
merged_df = None

for file in csv_files:
    df = pd.read_csv(file)

    # Standardize column names
    df.columns = df.columns.str.lower().str.strip()

    # Drop 'year' column if it exists
    df = df.drop(columns=['year'], errors='ignore')

    # Convert common columns to string to prevent merge type conflicts
    for col in common_columns:
        if col in df.columns:
            df[col] = df[col].astype(str)

    # Merge using full outer join
    if merged_df is None:
        merged_df = df
    else:
        merged_df = merged_df.merge(df, on=list(common_columns), how="outer")

print(f"\n‚úÖ Merged dataset created with shape: {merged_df.shape}\n")

# Step 3: Identify Columns Containing Years (2018-2024)
years_to_check = [str(year) for year in range(2018, 2025)]
yearly_datasets = {year: [] for year in years_to_check}

# Assign columns to their respective years while keeping common columns
col_assignment = {}  # Dictionary to track assigned columns

for col in merged_df.columns:
    # Extract all years mentioned in the column name
    found_years = sorted([year for year in years_to_check if year in col], key=lambda y: col.index(y))

    if found_years:
        # Assign the column to the dataset of the first year found
        correct_year = found_years[0]

        if col not in col_assignment:
            col_assignment[col] = correct_year
            yearly_datasets[correct_year].append(col)

# Step 4: Split the Dataset by Year and Verify Before Saving
for year, cols in yearly_datasets.items():
    if cols:
        # Keep common columns + year-specific columns
        final_cols = list(common_columns) + cols
        year_df = merged_df[final_cols].copy()

        # **Check which years are still in column names**
        found_years = [y for y in years_to_check if any(y in col for col in year_df.columns)]
        print(f"üìÇ Yearly dataset {year} contains columns with these years: {', '.join(found_years) if found_years else 'None'}")

        # Special check for 2018 dataset: Print columns containing "2019"
        if year == "2018":
            cols_with_2019 = [col for col in year_df.columns if "2019" in col]
            print(f"‚ö†Ô∏è Columns in yearly_data_2018.csv that contain '2019': {cols_with_2019}")

        # Save the dataset
        output_path = os.path.join(desktop_path, f"yearly_data_{year}.csv")
        year_df.to_csv(output_path, index=False)
        print(f"‚úÖ Saved {year}: {year_df.shape[0]} rows, {year_df.shape[1]} columns ‚Üí {output_path}")

print("\n‚úÖ All yearly datasets have been checked, verified, and saved on the desktop!")
