{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary packages\n",
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function 1: Loading in downloaded data to begin cleaning process \n",
    "def load_data(directory, year):\n",
    "    \"\"\"\n",
    "    Reads all CSV files in the specified directory into a dictionary (RAWDATA) \n",
    "    and loads all sheets from an Excel file into another dictionary (REF).\n",
    "    \n",
    "    Args:\n",
    "        directory (str): The path to the directory containing the raw data files.\n",
    "        year (int or str): The year to append to dictionary keys.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (RAWDATA, REF)\n",
    "        RAWDATA: Dictionary containing all raw data files \n",
    "        REF: Dictionary containing column IDs for each file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Change to the specified directory\n",
    "    os.chdir(directory)\n",
    "\n",
    "    # Store all raw CSV data in a dictionary with dynamic year\n",
    "    csv_files = [f for f in os.listdir() if f.endswith('.csv')]\n",
    "    RAWDATA = {file.split('.')[0].lower(): pd.read_csv(file) for file in csv_files}\n",
    "\n",
    "    # Look for an Excel file (assuming there's only one Excel file in the directory)\n",
    "    excel_files = [f for f in os.listdir() if f.endswith('.xlsx') or f.endswith('.xls')]\n",
    "    \n",
    "    REF = {}\n",
    "    if excel_files:\n",
    "        file_path = excel_files[0]  # Taking the first Excel file found\n",
    "        sheetname_ref = pd.ExcelFile(file_path).sheet_names\n",
    "        REF = {f\"{sheet}_ref{year}\": pd.read_excel(file_path, sheet_name=sheet) for sheet in sheetname_ref}\n",
    "\n",
    "    # Return both dictionaries\n",
    "    return RAWDATA, REF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function 2: Performing primary datacleaning of datasets including remove nas \n",
    "def primary_data_cleaning(df_dict, level):\n",
    "    \"\"\"\n",
    "    Conducts data cleaning on all raw dataframes. \n",
    "     \n",
    "    Converts all columns in each DataFrame (except the one containing the specified level) to numeric.\n",
    "    Replaces '.', '-1', and '-3' values with NaN.\n",
    "    \n",
    "    Additionally, searches the DataFrame's columns for a column that contains the specified level's long form\n",
    "    and renames it to '{original_name}_id' if found.\n",
    "    \n",
    "    Args:\n",
    "        df_dict (dict): Dictionary of pandas DataFrames.\n",
    "        level (str): Level of granularity ('C', 'D', 'R', 'S') corresponding to Campus, District, Region, State.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of DataFrames with processed data.\n",
    "    \"\"\"\n",
    "    level_map = {\n",
    "        \"C\": [\"Campus\", \"District\"],  # Campus level should include both Campus and District columns\n",
    "        \"D\": [\"District\"],\n",
    "        \"R\": [\"Region\"],\n",
    "        \"S\": [\"State\"]\n",
    "    }\n",
    "    \n",
    "    level_names = level_map.get(level)\n",
    "    if not level_names:\n",
    "        raise ValueError(\"Invalid level input. Must be one of 'C', 'D', 'R', 'S'.\")\n",
    "    \n",
    "    processed_dict = {}\n",
    "    \n",
    "    for key, df in df_dict.items():\n",
    "        # Skip processing for DataFrames whose key contains 'ref' or 'type' because they are different files \n",
    "        if 'ref' in key.lower() or 'type' in key.lower():\n",
    "            processed_dict[key] = df.copy()\n",
    "            continue\n",
    "        \n",
    "        df = df.copy()\n",
    "        \n",
    "        # Identify columns that match the specified level names\n",
    "        matching_columns = [col for col in df.columns if any(name.lower() in col.lower() for name in level_names)]\n",
    "        \n",
    "        # Rename matching columns by appending '_id'\n",
    "        for col in matching_columns:\n",
    "            df.rename(columns={col: f\"{col}_id\"}, inplace=True)\n",
    "        \n",
    "        # Convert all columns except the identified level columns to numeric\n",
    "        id_columns = [f\"{col}_id\" for col in matching_columns]\n",
    "        for col in df.columns:\n",
    "            if col not in id_columns:\n",
    "                df[col] = df[col].replace({'.': np.nan, '-1': np.nan, '-3': np.nan})  # Replace invalid values with NaN\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric\n",
    "            else:\n",
    "                df[col] = df[col].astype(str)  # Ensure ID columns remain as strings\n",
    "        \n",
    "        processed_dict[key] = df\n",
    "    \n",
    "    return processed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function 3: Using column refrence files to rename encoded files \n",
    "def rename_columns_using_ref(rawdata, ref):\n",
    "    \"\"\"\n",
    "    Renames columns in each DataFrame in rawdata using the corresponding mapping found in ref.\n",
    "    If a filename contains 'ref' or 'type', it is copied unchanged.\n",
    "\n",
    "    Args:\n",
    "        rawdata (dict): Dictionary containing raw DataFrames with keys as filenames.\n",
    "        ref (dict): Dictionary containing reference DataFrames with keys as filenames.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing renamed DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    updated_data = {}  # Dictionary to store updated DataFrames\n",
    "\n",
    "    for raw_key, raw_df in rawdata.items():\n",
    "        # Skip processing if key contains 'ref' or 'type'\n",
    "        if 'ref' in raw_key.lower() or 'type' in raw_key.lower():\n",
    "            updated_data[raw_key] = raw_df.copy()\n",
    "            continue\n",
    "        \n",
    "        # Extract base name before the first underscore (_)\n",
    "        base_name = raw_key.split(\"_\")[0]\n",
    "        \n",
    "        # Find the matching key in REF (case-insensitive)\n",
    "        matching_key = next((key for key in ref if key.lower().startswith(base_name.lower())), None)\n",
    "        \n",
    "        if matching_key:\n",
    "            # Extract mapping from REF (second column = column ID, third column = actual column name)\n",
    "            ref_df = ref[matching_key]\n",
    "            column_mapping = dict(zip(ref_df.iloc[:, 1], ref_df.iloc[:, 2]))  # Map column ID â†’ Actual name\n",
    "            \n",
    "            # Rename columns in RAWDATA DataFrame\n",
    "            renamed_df = raw_df.rename(columns=column_mapping)\n",
    "        else:\n",
    "            # If no match is found, keep the DataFrame unchanged\n",
    "            renamed_df = raw_df.copy()\n",
    "\n",
    "        # Store in updated_data with the original key\n",
    "        updated_data[raw_key] = renamed_df\n",
    "\n",
    "    # Print confirmation\n",
    "    print(f\"Processed {len(updated_data)} DataFrames (Renamed: {len([k for k in updated_data if k not in rawdata or ('ref' not in k.lower() and 'type' not in k.lower())])}, Unchanged: {len([k for k in updated_data if 'ref' in k.lower() or 'type' in k.lower()])}).\")\n",
    "\n",
    "    return updated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_reference(df_dict, level):\n",
    "    \"\"\"\n",
    "    Identifies the DataFrame with 'ref' in its key and left joins it with all other DataFrames \n",
    "    (except those containing '_type' in their key). The join is performed using {LEVEL} in the \n",
    "    reference DataFrame and f\"{LEVEL}_id\" in the other DataFrames.\n",
    "\n",
    "    Ensures that the merged columns are converted to strings before merging.\n",
    "\n",
    "    Args:\n",
    "        df_dict (dict): Dictionary of pandas DataFrames.\n",
    "        level (str): Level of granularity ('C', 'D', 'R', 'S') corresponding to Campus, District, Region, State.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated dictionary with joined data.\n",
    "    \"\"\"\n",
    "    level_full_name = {\n",
    "        \"C\": \"CAMPUS\",  \n",
    "        \"D\": \"DISTRICT\",\n",
    "        \"R\": \"REGION\",\n",
    "        \"S\": \"STATE\"\n",
    "    }[level]\n",
    "\n",
    "    ref_df = None\n",
    "    ref_key = None\n",
    "\n",
    "    # Locate the reference DataFrame\n",
    "    for key in df_dict.keys():\n",
    "        if 'ref' in key.lower():\n",
    "            ref_df = df_dict[key]\n",
    "            ref_key = key\n",
    "            break  # Only one reference DataFrame is assumed\n",
    "\n",
    "    if ref_df is None:\n",
    "        raise ValueError(\"No reference DataFrame found in the dictionary keys.\")\n",
    "\n",
    "    join_col_ref = level_full_name  # Column name in the reference DataFrame\n",
    "    join_col_main = f\"{level_full_name}_id\"  # Column name in other DataFrames\n",
    "\n",
    "    print(f\"Joining on: ref[{join_col_ref}] with main[{join_col_main}]\")\n",
    "\n",
    "    # Convert the reference column to string\n",
    "    ref_df[join_col_ref] = ref_df[join_col_ref].astype(str)\n",
    "\n",
    "    updated_dict = {}\n",
    "\n",
    "    for key, df in df_dict.items():\n",
    "        if key == ref_key or '_type' in key.lower():  \n",
    "            # Keep the reference and '_type' DataFrames unchanged\n",
    "            updated_dict[key] = df\n",
    "        elif join_col_main in df.columns:  \n",
    "            # Convert the main DataFrame's join column to string before merging\n",
    "            df[join_col_main] = df[join_col_main].astype(str)\n",
    "\n",
    "            # Perform the left join\n",
    "            updated_dict[key] = df.merge(ref_df, how='left', left_on=join_col_main, right_on=join_col_ref)\n",
    "        else:\n",
    "            # If no matching column, keep the DataFrame unchanged\n",
    "            updated_dict[key] = df\n",
    "\n",
    "    return updated_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_frames(dfs, level):\n",
    "    \"\"\"\n",
    "    Merges multiple pandas DataFrames stored in a dictionary, based on the given level.\n",
    "\n",
    "    - Uses the first DataFrame as the base for left joins.\n",
    "    - Drops shared columns (except for {level}_id) before merging.\n",
    "    - Merges any '_type' file (like 'district_type') last.\n",
    "    - Skips merging any DataFrame whose key contains \"ref\".\n",
    "    - Returns the original dictionary with the final merged DataFrame stored under the key \"merged\".\n",
    "\n",
    "    Parameters:\n",
    "        dfs (dict): A dictionary where keys are dataset names and values are pandas DataFrames.\n",
    "        level (str): One of 'C', 'D', or 'R' indicating Campus, District, or Region.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated dictionary with final merged DataFrame added under key \"merged\".\n",
    "    \"\"\"\n",
    "    level_full_name = {\n",
    "        \"C\": \"CAMPUS\",\n",
    "        \"D\": \"DISTRICT\",\n",
    "        \"R\": \"REGION\"\n",
    "    }[level]\n",
    "\n",
    "    id_col = f\"{level_full_name}_id\"\n",
    "\n",
    "    shared_columns = [\n",
    "        'DISTRICT', 'DISTNAME', 'COUNTY', 'CNTYNAME', 'REGION',\n",
    "        'DFLCHART', 'DFLALTED', 'D_RATING', 'OUTCOME', 'ASVAB_STATUS',\n",
    "        'asvab_status', 'DAD_POST', 'District Name'\n",
    "    ]\n",
    "\n",
    "    updated_dfs = {k: v.copy() for k, v in dfs.items()}\n",
    "    merge_df = list(updated_dfs.values())[0]\n",
    "    merged_keys = []\n",
    "\n",
    "    # Separate merge keys\n",
    "    all_keys = list(updated_dfs.keys())[1:]\n",
    "    regular_keys = [k for k in all_keys if \"ref\" not in k.lower() and \"district_type\" not in k.lower()]\n",
    "    type_key = next((k for k in all_keys if \"district_type\" in k.lower()), None)\n",
    "\n",
    "    # Merge regular files first\n",
    "    for key in regular_keys:\n",
    "        df_to_be_merged = updated_dfs[key].drop(columns=shared_columns, errors='ignore')\n",
    "        print(f\"\\nMerging {key}:\")\n",
    "        print(f\"  - Shape of merge_df before merge: {merge_df.shape}\")\n",
    "        print(f\"  - Shape of df_to_be_merged: {df_to_be_merged.shape}\")\n",
    "\n",
    "        merge_df = merge_df.merge(df_to_be_merged, on=id_col, how=\"left\")\n",
    "        print(f\"  - Merged with LEFT join on '{id_col}'. New shape: {merge_df.shape}\")\n",
    "        merged_keys.append(key)\n",
    "\n",
    "    # Merge the _type file last (only for District level)\n",
    "    if type_key and level == \"D\":\n",
    "        df_to_be_merged = updated_dfs[type_key].drop(columns=shared_columns, errors='ignore')\n",
    "        print(f\"\\nMerging {type_key} (merged last):\")\n",
    "        print(f\"  - Shape of merge_df before merge: {merge_df.shape}\")\n",
    "        print(f\"  - Shape of df_to_be_merged: {df_to_be_merged.shape}\")\n",
    "\n",
    "        merge_df[id_col] = merge_df[id_col].astype(str).str.replace(r\"[^\\d]\", \"\", regex=True).astype(int)\n",
    "        df_to_be_merged[\"District Number\"] = df_to_be_merged[\"District Number\"].astype(int)\n",
    "        merge_df = merge_df.merge(df_to_be_merged, left_on=id_col, right_on=\"District Number\", how=\"left\")\n",
    "        print(f\"  - Merged with LEFT join on '{id_col}' and 'District Number'. New shape: {merge_df.shape}\")\n",
    "        merged_keys.append(type_key)\n",
    "\n",
    "    # Store final result\n",
    "    if merged_keys:\n",
    "        updated_dfs[\"merged\"] = merge_df\n",
    "    else:\n",
    "        print(\"No merges were performed.\")\n",
    "\n",
    "    return updated_dfs\n",
    "\n",
    "\n",
    "# Helper Function 5: Combining the previous 4 functions in 1 to fully process the data in python environment \n",
    "def processing(directory, year, level):\n",
    "    \"\"\"\n",
    "    Processes raw data by loading, cleaning, renaming columns, and joining level refrence.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing the data.\n",
    "        year (int): Year of the data to be processed.\n",
    "        level (str): Cleaning level or category for data processing.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The cleaned and processed data with renamed columns.\n",
    "    \"\"\"\n",
    "    rawdata, ref = load_data(directory, year)\n",
    "    cleaned_data = primary_data_cleaning(rawdata, level)\n",
    "    column_data = rename_columns_using_ref(cleaned_data, ref)\n",
    "    if level != 'S':\n",
    "        ref_data = join_with_reference(column_data, level)\n",
    "        final_data = merge_data_frames(ref_data, level)\n",
    "        return final_data\n",
    "    else:\n",
    "        return column_data\n",
    "\n",
    "# Master Function: Process data and store it into a specified directory. \n",
    "def process_and_save_all_data(base_directory, level):\n",
    "    \"\"\"\n",
    "    Loops through all Data{year} folders, processes the data, and saves the output\n",
    "    as multiple Excel files in the corresponding clean_data folder within each level.\n",
    "    \n",
    "    Parameters:\n",
    "    base_directory (str): Path to the folder containing Data{year} folders.\n",
    "    level (str): Level parameter required for data processing.\n",
    "    \"\"\"\n",
    "    # Get the full level name \n",
    "    valid_levels = {\n",
    "        'C': 'Campus',\n",
    "        'D': 'District',\n",
    "        'R': 'Region',\n",
    "        'S': 'State'\n",
    "    }\n",
    "\n",
    "    # Get all folder names and extract years\n",
    "    year_folders = [f for f in os.listdir(base_directory) if f.startswith('Data')]\n",
    "    years = sorted([int(f.replace('Data', '')) for f in year_folders if f.replace('Data', '').isdigit() and int(f.replace('Data', '')) >= 2020])\n",
    "    \n",
    "    for year in years:\n",
    "        data_year_folder = os.path.join(base_directory, f'Data{year}')\n",
    "        raw_data_folder = os.path.join(data_year_folder, f'{valid_levels[level]}', 'raw_data')\n",
    "        clean_data_folder = os.path.join(data_year_folder, f'{valid_levels[level]}', 'clean_data')\n",
    "        os.makedirs(clean_data_folder, exist_ok=True)\n",
    "        \n",
    "        if os.path.exists(raw_data_folder):\n",
    "            # Check if all output files already exist before processing\n",
    "            processed_data_needed = False\n",
    "            \n",
    "            for file_name in os.listdir(raw_data_folder):\n",
    "                clean_file_name = f\"{os.path.splitext(file_name)[0]}_clean.csv\"\n",
    "                output_file = os.path.join(clean_data_folder, clean_file_name)\n",
    "                if not os.path.exists(output_file):\n",
    "                    processed_data_needed = True\n",
    "                    break\n",
    "            \n",
    "            if not processed_data_needed:\n",
    "                print(f\"Skipping processing for year {year} at level {level} as all files already exist.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing data for year {year} at level {level}...\")\n",
    "            \n",
    "            # Run the processing function, which returns a dictionary of DataFrames\n",
    "            processed_data = processing(raw_data_folder, year, level)\n",
    "            \n",
    "            # Save each DataFrame in the dictionary as a separate Excel file\n",
    "            for file_name, df in processed_data.items():\n",
    "                clean_file_name = f\"{file_name}_clean.csv\"\n",
    "                output_file = os.path.join(clean_data_folder, clean_file_name)\n",
    "                \n",
    "                if os.path.exists(output_file):\n",
    "                    print(f\"Skipping {clean_file_name} as it already exists.\")\n",
    "                else:\n",
    "                    df.to_csv(output_file, index=False)\n",
    "                    print(f\"Saved cleaned data for {year}, level {level}: {clean_file_name}\")\n",
    "        else:\n",
    "            print(f\"Warning: Raw data folder for {year} at level {level} does not exist, skipping...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for year 2023 at level C...\n",
      "Processed 2 DataFrames (Renamed: 1, Unchanged: 1).\n",
      "Joining on: ref[CAMPUS] with main[CAMPUS_id]\n",
      "No merges were performed.\n",
      "Saved cleaned data for 2023, level C: campgrad_2023_clean.csv\n",
      "Saved cleaned data for 2023, level C: cref_2023_clean.csv\n"
     ]
    }
   ],
   "source": [
    "process_and_save_all_data(r\"C:\\Users\\mmath\\OneDrive\\Desktop\\Scraper Testing\", 'C')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
