{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Texas Academic Performance Report (TAPR) Data Scraper\n",
    "This scraper allows users to select the level (Campus, District, Region, State) and type of data they would like to download from the TAPR data download on the TEA website. If the level is \"D\" for District, district type data will also be downloaded in addition to the TAPR data unless the user has indicated they do not want the data (set dist_type = False). \n",
    "\n",
    "If the files already exist, the scraper will not download new files. \n",
    "\n",
    "The scraper creates separate folders for each year of data and names the files with the appropriate year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "District Type Scraper (Helper Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def district_type_scraper(year):\n",
    "   \"\"\"\n",
    "   Scrapes the Texas Education Agency (TEA) website for district type data of a given school year.\n",
    "\n",
    "   Args:\n",
    "      year (int): The ending year of the school year (e.g., 2024 for the 2023-24 school year).\n",
    "\n",
    "   Returns:\n",
    "      pd.DataFrame: A DataFrame containing data from the specified school year's district type Excel file.\n",
    "\n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If there's an issue fetching the webpage.\n",
    "        ValueError: If no Excel file is found on the page.   \n",
    "   \"\"\"\n",
    "   school_year = str(year-1)+\"-\"+str(year-2000) # Finds the academic school year \n",
    "   #Web scraping academic year for district data from website \n",
    "   url = f'https://tea.texas.gov/reports-and-data/school-data/district-type-data-search/district-type-{school_year}'\n",
    "   grab = requests.get(url)\n",
    "   soup = BeautifulSoup(grab.text, 'html.parser')\n",
    "   xlsx = []\n",
    "   for link in soup.find_all(\"a\"):\n",
    "      data = str(link.get('href'))\n",
    "      if re.search(\".xlsx$\", data):\n",
    "         return pd.read_excel(f\"https://tea.texas.gov{data}\", sheet_name= 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detects if files download within a given time limit, otherwise timesout (Helper Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_downloads(variables, year, directory, level, timeout=200):\n",
    "    \"\"\"\n",
    "    Waits for the expected data files to be downloaded within a specified timeout period.\n",
    "\n",
    "    Args:\n",
    "        variables (list of str): A list of variable names that determine expected file names.\n",
    "        year (int): The year of the dataset, affecting file format (.dat for <2021, .csv for >=2021).\n",
    "        directory (str): The directory where the files are expected to be downloaded.\n",
    "        timeout (int, optional): Maximum time in seconds to wait for all files to be downloaded. Defaults to 200.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all expected files are downloaded within the timeout period, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified directory does not exist.\n",
    "\n",
    "    Notes:\n",
    "        - The function checks for `.crdownload` files to ensure downloads are complete.\n",
    "        - It waits in 5-second intervals before checking again.\n",
    "        - If downloads complete within the timeout, a success message is printed.\n",
    "    \"\"\"\n",
    "    file_prefix = {\n",
    "        'C': 'CAMP',\n",
    "        'D': 'DIST',\n",
    "        'R': 'REGN',\n",
    "        'S': 'STATE'\n",
    "    }[level]\n",
    "\n",
    "    start_time = time.time()  # Record the start time\n",
    "    expected_files = []  # List to store expected file names based on year and variables\n",
    "\n",
    "    # Determine expected file names based on the year\n",
    "    for var in variables:\n",
    "        if year < 2021:\n",
    "            expected_files.append(f\"{file_prefix}{var}.dat\" if var != \"REF\" else f\"{level}REF.dat\")\n",
    "        else:\n",
    "            expected_files.append(f\"{file_prefix}{var}.csv\" if var != \"REF\" else f\"{level}REF.csv\")\n",
    "\n",
    "    check = 1  # Variable to print waiting message only once\n",
    "    while time.time() - start_time < timeout:  # Continue checking until timeout is reached\n",
    "        downloaded_files = os.listdir(directory)  # Get the list of files in the directory\n",
    "\n",
    "        # Check if all expected files are present and not still downloading (.crdownload files)\n",
    "        if all(file in downloaded_files and not file.endswith(\".crdownload\") for file in expected_files):\n",
    "            print(f\"All downloads for {year} completed successfully.\\n\")\n",
    "            return True  # Return True if all files are found and fully downloaded\n",
    "        \n",
    "        # Print waiting message only once at the start\n",
    "        if check == 1:\n",
    "            print(\"Waiting for all files to download...\")\n",
    "        check += 1\n",
    "\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "\n",
    "    return False  # Return False if the timeout is reached before all files are downloaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renames files to include year in file name (Helper Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_renamer(directory, year, prefix, var, level):\n",
    "    \"\"\"\n",
    "    Renames downloaded files in the specified directory based on naming conventions.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The path to the directory containing the files.\n",
    "        year (int): The year to be appended to the renamed files.\n",
    "        prefix (str): The prefix used in some file names (e.g., 'DIST').\n",
    "        var (str): The variable name (e.g., 'POP', 'ECON', 'REF').\n",
    "        level (str): The level identifier (some files may use this instead of the prefix).\n",
    "\n",
    "    Returns:\n",
    "        None: The function renames files in place and does not return a value.\n",
    "\n",
    "    Notes:\n",
    "        - The function checks for `.csv` and `.dat` file extensions.\n",
    "        - It looks for two possible naming patterns:\n",
    "            1. `{prefix}{var}{ext}` (e.g., `DISTPOP.csv`)\n",
    "            2. `{level}{var}{ext}` (e.g., `STATEPOP.dat`)\n",
    "        - If a match is found, the file is renamed to:\n",
    "            - `{level}{var}_{year}{ext}` for \"REF\" files.\n",
    "            - `{prefix}{var}_{year}{ext}` for all other files.\n",
    "        - The function **only renames the first matching file** and stops checking further.\n",
    "    \"\"\"\n",
    "    for ext in ['.csv', '.dat']:  # Check both CSV and DAT file formats\n",
    "        old_patterns = [\n",
    "            f\"{prefix}{var}{ext}\",  # Pattern with prefix\n",
    "            f\"{level}{var}{ext}\"     # Pattern with level (some files may not have prefix)\n",
    "        ]\n",
    "\n",
    "        for old_pattern in old_patterns:\n",
    "            old_name = os.path.join(directory, old_pattern)  # Full path of the old file\n",
    "            if os.path.exists(old_name):  # Check if file exists\n",
    "                # Determine new name format\n",
    "                if var == \"REF\":\n",
    "                    new_name = os.path.join(directory, f\"{level}{var}_{year}{ext}\")\n",
    "                else:\n",
    "                    new_name = os.path.join(directory, f\"{prefix}{var}_{year}{ext}\")\n",
    "\n",
    "                os.rename(old_name, new_name)  # Rename the file\n",
    "                break  # Stop checking after renaming the first matching file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts .dat files to .csv files automatically (Helper Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dat_to_csv(directory):\n",
    "    \"\"\"\n",
    "    Converts all .dat files in the specified directory to .csv files and deletes the .dat files after conversion.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): Path to the directory containing .dat files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Iterate through files in the directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".dat\"):\n",
    "            dat_file_path = os.path.join(directory, file_name)\n",
    "            csv_file_path = os.path.join(directory, file_name.replace(\".dat\", \".csv\"))\n",
    "\n",
    "            try:\n",
    "                # Read the .dat file with automatic delimiter detection\n",
    "                df = pd.read_csv(dat_file_path, delimiter=None, engine='python')\n",
    "\n",
    "                # Save as .csv\n",
    "                df.to_csv(csv_file_path, index=False)\n",
    "                print(f\"Converted: {file_name} -> {csv_file_path}\")\n",
    "\n",
    "                # Delete the original .dat file after successful conversion\n",
    "                os.remove(dat_file_path)\n",
    "                print(f\"Deleted original file: {file_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Refrence Files and Download Them (HELPER FUNCTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tea_reference_scraper(directory_path, year, level):\n",
    "    \"\"\"\n",
    "    Scrape reference files for a specified year and level of data.\n",
    "    \n",
    "    Parameters:\n",
    "        directory_path (str): File path where data should be downloaded.\n",
    "        year (int): Year to scrape data for (formatted YYYY).\n",
    "        level (str): Administrative level to scrape. Options:\n",
    "            'C' -> 'campus', 'D' -> 'district', 'S' -> 'state', 'R' -> 'region'\n",
    "    \n",
    "    Returns: \n",
    "        Specified files stored in the given directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Map shorthand inputs to full level names\n",
    "    level_map = {'C': 'campus', 'D': 'district', 'S': 'state', 'R': 'region'}\n",
    "    \n",
    "    if level not in level_map:\n",
    "        raise ValueError(f\"Invalid level. Must be one of: {', '.join(level_map.keys())}\")\n",
    "    \n",
    "    full_level_name = level_map[level]\n",
    "    \n",
    "    # Construct URL for TAPR data download page\n",
    "    url = f\"https://rptsvr1.tea.texas.gov/perfreport/tapr/{year}/download/DownloadData.html\"\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    \n",
    "    # Set up Chrome WebDriver options\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    absolute_download_path = os.path.abspath(directory_path)\n",
    "    \n",
    "    # Configure download preferences to automate file saving\n",
    "    prefs = {\n",
    "        \"download.default_directory\": absolute_download_path,\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"safebrowsing.enabled\": True,\n",
    "        \"profile.default_content_settings.popups\": 0\n",
    "    }\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize web scraper and navigate to the data download page\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        # **Add a delay before accessing the page**\n",
    "        time.sleep(random.uniform(3, 7))  # Wait between 3 to 7 seconds\n",
    "        driver.get(url)\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Check if the page loaded properly\n",
    "        if \"Page Not Found\" in driver.page_source or \"404\" in driver.page_source:\n",
    "            print(f\"Year {year} does not exist. Skipping...\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Downloading reference file for {full_level_name} level, {year}...\")\n",
    "        file_name = f\"TAPR_{full_level_name}_adv_{year}.xlsx\"\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        \n",
    "        # Skip downloading if the file already exists\n",
    "        if os.path.isfile(file_path):\n",
    "            print(f\"{file_name} already exists\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Find and click the link that corresponds to the required level\n",
    "            link = driver.find_element(By.XPATH, f\"//a[contains(text(), '{full_level_name}')]\")\n",
    "            link.click()\n",
    "            print(f\"Download initiated for {file_name}\")\n",
    "            \n",
    "        except NoSuchElementException:\n",
    "            print(f\"Reference file link not found for {year}\")\n",
    "            return\n",
    "        \n",
    "        # Wait for the download to complete with an improved wait strategy\n",
    "        download_timeout = 10  # Max wait time in seconds\n",
    "        elapsed_time = 0\n",
    "        while not os.path.isfile(file_path) and elapsed_time < download_timeout:\n",
    "            time.sleep(1)\n",
    "            elapsed_time += 1\n",
    "        \n",
    "        if not os.path.isfile(file_path):\n",
    "            print(f\"Download did not complete in time for {file_name}\")\n",
    "        \n",
    "    except WebDriverException as e:\n",
    "        print(f\"Failed to access {year}. Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    print(\"Reference file download complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Data from TAPR Advanced Download (Master Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tea_scraper(directory_path, years, variables, level, dist_type=True):\n",
    "    \"\"\"\n",
    "    Scrape all HERC data for specified years, variables, and level of data.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (string): file path that you would like data to be downloaded to\n",
    "        years (list): List of years to scrape data for (formatted YYYY)\n",
    "        variables (list): List of variable codes to download (such as \"GRAD\")\n",
    "        level (str): Administrative level to scrape. Options:\n",
    "            'C' for Campus\n",
    "            'D' for District\n",
    "            'R' for Region\n",
    "            'S' for State\n",
    "\n",
    "    Returns: \n",
    "        Specified files stored in folders located in users current directory. \n",
    "    \"\"\"\n",
    "    #Appending REF files to the selected variables. This helps you encode the campus, district, and region ids\n",
    "    variables.append('REF')\n",
    "\n",
    "    ### Checking to see if directory is a valid path and continuing code if it is  ### \n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"Error: {directory_path} is not a valid directory.\")\n",
    "        return\n",
    "    print(f\"Processing directory: {directory_path}\")\n",
    "    directory_path_name = directory_path\n",
    "\n",
    "    ### Evaluating if level is accurate or not and converting it to expanded level name ### \n",
    "    valid_levels = {\n",
    "        'C': 'Campus',\n",
    "        'D': 'District',\n",
    "        'R': 'Region',\n",
    "        'S': 'State'\n",
    "    }\n",
    "    \n",
    "    if level not in valid_levels:\n",
    "        raise ValueError(f\"Invalid level. Must be one of: {', '.join(valid_levels.keys())}\")\n",
    "    \n",
    "    file_prefix = {\n",
    "        'C': 'CAMP',\n",
    "        'D': 'DIST',\n",
    "        'R': 'REGN',\n",
    "        'S': 'STATE'\n",
    "    }[level]\n",
    "\n",
    "    ### Looping through years to get data ### \n",
    "    for year in years:\n",
    "        #Construct URL for TAPR data download page \n",
    "        url = f\"https://rptsvr1.tea.texas.gov/perfreport/tapr/{year}/download/DownloadData.html\"\n",
    "\n",
    "        #Create a directory for current years data \n",
    "        #dir_name = f\"raw_data{year}\"\n",
    "        full_dir_path = os.path.join(directory_path_name, f\"Data{year}\", f\"{valid_levels[level]}\", f\"raw_data\")\n",
    "        os.makedirs(full_dir_path, exist_ok=True)\n",
    "        \n",
    "        #Set up chrome webdriver options \n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        absolute_download_path = os.path.abspath(full_dir_path)\n",
    "        \n",
    "        #Configure download prefrences to automate file saving \n",
    "        prefs = {\n",
    "            \"download.default_directory\": absolute_download_path,\n",
    "            \"download.prompt_for_download\": False,\n",
    "            \"download.directory_upgrade\": True,\n",
    "            \"safebrowsing.enabled\": True,\n",
    "            \"profile.default_content_settings.popups\": 0\n",
    "        }\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "        try:\n",
    "            #Initialive web scraper and navigate to the data download page \n",
    "            driver = webdriver.Chrome(options=chrome_options)\n",
    "            driver.get(url)\n",
    "\n",
    "            # Check if the page loaded properly\n",
    "            if \"Page Not Found\" in driver.page_source or \"404\" in driver.page_source:\n",
    "                print(f\"Year {year} does not exist. Skipping...\")\n",
    "                driver.quit()\n",
    "                continue  # Skip this year and move to the next one\n",
    "            #Select the desired level \n",
    "            level_select = driver.find_element(By.XPATH, f\"//input[@type='radio' and @name='sumlev' and @value='{level}']\")\n",
    "            level_select.click()\n",
    "        \n",
    "        except WebDriverException as e:\n",
    "            print(f\"Failed to access {year}. Error: {e}\")\n",
    "            driver.quit()\n",
    "            continue  # Skip this year\n",
    "\n",
    "        unavailable = [] #Track unavilable files \n",
    "        print(f\"Downloading {valid_levels[level]} Level TAPR Data for {year}...\")\n",
    "        \n",
    "        # Loop through variables to download corresponding files \n",
    "        for var in variables:\n",
    "            print(f\"Checking for {file_prefix}{var} data...\")\n",
    "            #Define possible file patters to check if file is there \n",
    "            file_patterns = [\n",
    "                f\"{file_prefix}{var}_{year}.csv\",\n",
    "                f\"{file_prefix}{var}_{year}.dat\",\n",
    "                f\"{level}{var}_{year}.dat\",\n",
    "                f\"{level}{var}_{year}.csv\"\n",
    "            ]\n",
    "            \n",
    "            # Skip downloading if the file already exists \n",
    "            if any(os.path.isfile(os.path.join(full_dir_path, file)) for file in file_patterns):\n",
    "                print(f\"{var}_{year} already exists\")\n",
    "                unavailable.append(var)\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                #Select the dataset corresponding to the current variables \n",
    "                select_data = driver.find_element(By.XPATH, f\"//input[@type='radio' and @name='setpick' and @value='{var}']\")\n",
    "                select_data.click()\n",
    "                \n",
    "                time.sleep(1)\n",
    "                #Click continue button to initiate download \n",
    "                download = driver.find_element(By.XPATH, \"//input[@type='submit' and @value='Continue']\")\n",
    "                download.click()\n",
    "                print(f\"Downloaded {level if var == 'REF' else file_prefix}{var} for {year}\")\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                print(f\"{var} not found for {year}\")\n",
    "                unavailable.append(var)\n",
    "                continue\n",
    "        \n",
    "        #Get the list of successfully downloaded variables \n",
    "        available_vars = set(variables) - set(unavailable)\n",
    "\n",
    "        ### REFRENCE FILE DATA DOWNLOAD: Calling helper function to download refrence file data\n",
    "        tea_reference_scraper(full_dir_path, year, level)\n",
    "\n",
    "        #Wait for all downlods to complet and rename files accordingly \n",
    "        if wait_for_downloads(variables=available_vars, year=year, directory=full_dir_path, level = level):\n",
    "            for a_var in available_vars:\n",
    "                file_renamer(directory=full_dir_path, year=year, prefix=file_prefix, var=a_var, level=level)  \n",
    "        driver.quit()\n",
    "\n",
    "        #Calling helper function to convert .dat files to .csv files \n",
    "        convert_dat_to_csv(full_dir_path)\n",
    "\n",
    "        #If downloading district level data, get the district type dataset \n",
    "        if level == \"D\" and dist_type:\n",
    "            print(f\"Downloading District Type Data for {year}...\")\n",
    "            \n",
    "            if os.path.isfile(os.path.join(full_dir_path, f\"district_type{year}.csv\")):\n",
    "                print(f\"District Type Data for {year} already exists\")\n",
    "                print(\"\")\n",
    "                continue\n",
    "\n",
    "            df = district_type_scraper(year)\n",
    "\n",
    "            if df is None:\n",
    "                print(f\"Failed to retrieve District Type Data for {year}. Skipping...\")\n",
    "                continue  # Skip this year\n",
    "\n",
    "            df.to_csv(os.path.join(full_dir_path, f\"district_type{year}.csv\"), index=False)\n",
    "            print(f\"Downloaded District Type Data for {year}\")\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "    print(\"All Data Downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: C:\\Users\\mmath\\OneDrive\\Desktop\\Capstone\\HERC_Sp25\\Data\n",
      "Downloading District Level TAPR Data for 2020...\n",
      "Checking for DISTSTAAR1 data...\n",
      "STAAR1_2020 already exists\n",
      "Checking for DISTREF data...\n",
      "REF_2020 already exists\n",
      "Downloading reference file for district level, 2020...\n",
      "TAPR_district_adv_2020.xlsx already exists\n",
      "All downloads for 2020 completed successfully.\n",
      "\n",
      "Downloading District Type Data for 2020...\n",
      "District Type Data for 2020 already exists\n",
      "\n",
      "Downloading District Level TAPR Data for 2021...\n",
      "Checking for DISTSTAAR1 data...\n",
      "STAAR1_2021 already exists\n",
      "Checking for DISTREF data...\n",
      "REF_2021 already exists\n",
      "Downloading reference file for district level, 2021...\n",
      "TAPR_district_adv_2021.xlsx already exists\n",
      "All downloads for 2021 completed successfully.\n",
      "\n",
      "Downloading District Type Data for 2021...\n",
      "District Type Data for 2021 already exists\n",
      "\n",
      "Downloading District Level TAPR Data for 2022...\n",
      "Checking for DISTSTAAR1 data...\n",
      "STAAR1_2022 already exists\n",
      "Checking for DISTREF data...\n",
      "REF_2022 already exists\n",
      "Downloading reference file for district level, 2022...\n",
      "TAPR_district_adv_2022.xlsx already exists\n",
      "All downloads for 2022 completed successfully.\n",
      "\n",
      "Downloading District Type Data for 2022...\n",
      "District Type Data for 2022 already exists\n",
      "\n",
      "All Data Downloaded!\n"
     ]
    }
   ],
   "source": [
    "# Run the function with all currently available years and all the TAPR datasets\n",
    "\n",
    "#tea_scraper(r\"C:\\Users\\mmath\\OneDrive\\Desktop\\Capstone\\HERC_Sp25\\Data\",\n",
    "#[2020, 2021, 2022], ['STAAR1'], 'D')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
