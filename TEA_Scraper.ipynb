{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Texas Academic Performance Report (TAPR) Data Scraper\n",
    "This scraper allows users to select the level (Campus, District, Region, State) and type of data they would like to download from the TAPR data download on the TEA website. If the level is \"D\" for District, district type data will also be downloaded in addition to the TAPR data unless the user has indicated they do not want the data (set dist_type = False). \n",
    "\n",
    "If the files already exist, the scraper will not download new files. \n",
    "\n",
    "The scraper creates separate folders for each year of data and names the files with the appropriate year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "District Type Scraper (Helper Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def district_type_scraper(year):\n",
    "   \"\"\"\n",
    "   Scrapes the Texas Education Agency (TEA) website for district type data of a given school year.\n",
    "\n",
    "   Args:\n",
    "      year (int): The ending year of the school year (e.g., 2024 for the 2023-24 school year).\n",
    "\n",
    "   Returns:\n",
    "      pd.DataFrame: A DataFrame containing data from the specified school year's district type Excel file.\n",
    "\n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If there's an issue fetching the webpage.\n",
    "        ValueError: If no Excel file is found on the page.   \n",
    "   \"\"\"\n",
    "   school_year = str(year-1)+\"-\"+str(year-2000) # Finds the academic school year \n",
    "   #Web scraping academic year for district data from website \n",
    "   url = f'https://tea.texas.gov/reports-and-data/school-data/district-type-data-search/district-type-{school_year}'\n",
    "   grab = requests.get(url)\n",
    "   soup = BeautifulSoup(grab.text, 'html.parser')\n",
    "   xlsx = []\n",
    "   for link in soup.find_all(\"a\"):\n",
    "      data = str(link.get('href'))\n",
    "      if re.search(\".xlsx$\", data):\n",
    "         return pd.read_excel(f\"https://tea.texas.gov{data}\", sheet_name= 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detects if files download within a given time limit, otherwise timesout (Helper Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_downloads(variables, year, directory, timeout=200):\n",
    "    \"\"\"\n",
    "    Waits for the expected data files to be downloaded within a specified timeout period.\n",
    "\n",
    "    Args:\n",
    "        variables (list of str): A list of variable names that determine expected file names.\n",
    "        year (int): The year of the dataset, affecting file format (.dat for <2021, .csv for >=2021).\n",
    "        directory (str): The directory where the files are expected to be downloaded.\n",
    "        timeout (int, optional): Maximum time in seconds to wait for all files to be downloaded. Defaults to 200.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all expected files are downloaded within the timeout period, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified directory does not exist.\n",
    "\n",
    "    Notes:\n",
    "        - The function checks for `.crdownload` files to ensure downloads are complete.\n",
    "        - It waits in 5-second intervals before checking again.\n",
    "        - If downloads complete within the timeout, a success message is printed.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Record the start time\n",
    "    expected_files = []  # List to store expected file names based on year and variables\n",
    "\n",
    "    # Determine expected file names based on the year\n",
    "    for var in variables:\n",
    "        if year < 2021:\n",
    "            expected_files.append(f\"DIST{var}.dat\" if var != \"REF\" else \"DREF.dat\")\n",
    "        else:\n",
    "            expected_files.append(f\"DIST{var}.csv\" if var != \"REF\" else \"DREF.csv\")\n",
    "\n",
    "    check = 1  # Variable to print waiting message only once\n",
    "    while time.time() - start_time < timeout:  # Continue checking until timeout is reached\n",
    "        downloaded_files = os.listdir(directory)  # Get the list of files in the directory\n",
    "\n",
    "        # Check if all expected files are present and not still downloading (.crdownload files)\n",
    "        if all(file in downloaded_files and not file.endswith(\".crdownload\") for file in expected_files):\n",
    "            print(f\"All downloads for {year} completed successfully.\\n\")\n",
    "            return True  # Return True if all files are found and fully downloaded\n",
    "        \n",
    "        # Print waiting message only once at the start\n",
    "        if check == 1:\n",
    "            print(\"Waiting for all files to download...\")\n",
    "        check += 1\n",
    "\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "\n",
    "    return False  # Return False if the timeout is reached before all files are downloaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renames files to include year in file name (Helper Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_renamer(directory, year, prefix, var, level):\n",
    "    \"\"\"\n",
    "    Renames downloaded files in the specified directory based on naming conventions.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The path to the directory containing the files.\n",
    "        year (int): The year to be appended to the renamed files.\n",
    "        prefix (str): The prefix used in some file names (e.g., 'DIST').\n",
    "        var (str): The variable name (e.g., 'POP', 'ECON', 'REF').\n",
    "        level (str): The level identifier (some files may use this instead of the prefix).\n",
    "\n",
    "    Returns:\n",
    "        None: The function renames files in place and does not return a value.\n",
    "\n",
    "    Notes:\n",
    "        - The function checks for `.csv` and `.dat` file extensions.\n",
    "        - It looks for two possible naming patterns:\n",
    "            1. `{prefix}{var}{ext}` (e.g., `DISTPOP.csv`)\n",
    "            2. `{level}{var}{ext}` (e.g., `STATEPOP.dat`)\n",
    "        - If a match is found, the file is renamed to:\n",
    "            - `{level}{var}_{year}{ext}` for \"REF\" files.\n",
    "            - `{prefix}{var}_{year}{ext}` for all other files.\n",
    "        - The function **only renames the first matching file** and stops checking further.\n",
    "    \"\"\"\n",
    "    for ext in ['.csv', '.dat']:  # Check both CSV and DAT file formats\n",
    "        old_patterns = [\n",
    "            f\"{prefix}{var}{ext}\",  # Pattern with prefix\n",
    "            f\"{level}{var}{ext}\"     # Pattern with level (some files may not have prefix)\n",
    "        ]\n",
    "\n",
    "        for old_pattern in old_patterns:\n",
    "            old_name = os.path.join(directory, old_pattern)  # Full path of the old file\n",
    "            if os.path.exists(old_name):  # Check if file exists\n",
    "                # Determine new name format\n",
    "                if var == \"REF\":\n",
    "                    new_name = os.path.join(directory, f\"{level}{var}_{year}{ext}\")\n",
    "                else:\n",
    "                    new_name = os.path.join(directory, f\"{prefix}{var}_{year}{ext}\")\n",
    "\n",
    "                os.rename(old_name, new_name)  # Rename the file\n",
    "                break  # Stop checking after renaming the first matching file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts .dat files to .csv files automatically (Helper Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function: Converts .dat files to .csv files \n",
    "def convert_dat_to_csv(directory):\n",
    "    \"\"\"\n",
    "    Converts all .dat files in the specified directory to .csv files.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): Path to the directory containing .dat files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Iterate through files in the directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".dat\"):\n",
    "            dat_file_path = os.path.join(directory, file_name)\n",
    "            csv_file_path = os.path.join(directory, file_name.replace(\".dat\", \".csv\"))\n",
    "\n",
    "            try:\n",
    "                # Read the .dat file with automatic delimiter detection\n",
    "                df = pd.read_csv(dat_file_path, delimiter=None, engine='python')\n",
    "\n",
    "                # Save as .csv\n",
    "                df.to_csv(csv_file_path, index=False)\n",
    "                print(f\"Converted: {file_name} -> {csv_file_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {file_name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Data from TAPR Advanced Download (Master Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tea_scraper(years, variables, level, dist_type = True):\n",
    "    \"\"\"\n",
    "    Scrape all HERC data for specified years, variables, and level of data.\n",
    "    \n",
    "    Parameters:\n",
    "        years (list): List of years to scrape data for (formatted YYYY)\n",
    "        variables (list): List of variable codes to download (such as \"GRAD\")\n",
    "        level (str): Administrative level to scrape. Options:\n",
    "            'C' for Campus\n",
    "            'D' for District\n",
    "            'R' for Region\n",
    "            'S' for State\n",
    "\n",
    "    Returns: \n",
    "        Specified files stored in folders located in users current directory. \n",
    "    \"\"\"\n",
    "    directory_path_name = os.getcwd()\n",
    "    # Validation for level parameter\n",
    "    valid_levels = {\n",
    "        'C': 'Campus',\n",
    "        'D': 'District',\n",
    "        'R': 'Region',\n",
    "        'S': 'State'\n",
    "    }\n",
    "    \n",
    "    if level not in valid_levels:\n",
    "        raise ValueError(f\"Invalid level. Must be one of: {', '.join(valid_levels.keys())}\")\n",
    "    \n",
    "    # Create prefix for filenames based on level\n",
    "    file_prefix = {\n",
    "        'C': 'CAMP',\n",
    "        'D': 'DIST',\n",
    "        'R': 'REGN',\n",
    "        'S': 'STATE'\n",
    "    }[level]\n",
    "    \n",
    "    for year in years:\n",
    "        ### TAPR DATA DOWNLOAD ###\n",
    "        # Create full path for year directory\n",
    "        dir_name = f\"raw_data{year}\"\n",
    "        full_dir_path = os.path.join(directory_path_name, dir_name)\n",
    "        os.makedirs(full_dir_path, exist_ok=True)\n",
    "        \n",
    "        # Configure Chrome options\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        absolute_download_path = os.path.abspath(full_dir_path)\n",
    "        \n",
    "        # Add additional Chrome preferences to prevent download prompts\n",
    "        prefs = {\n",
    "            \"download.default_directory\": absolute_download_path,\n",
    "            \"download.prompt_for_download\": False,\n",
    "            \"download.directory_upgrade\": True,\n",
    "            \"safebrowsing.enabled\": True,\n",
    "            \"profile.default_content_settings.popups\": 0\n",
    "        }\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        # Add additional Chrome arguments\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        \n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(f\"https://rptsvr1.tea.texas.gov/perfreport/tapr/{year}/download/DownloadData.html\")\n",
    "        \n",
    "        # Select appropriate level\n",
    "        level_select = driver.find_element(By.XPATH, f\"//input[@type='radio' and @name='sumlev' and @value='{level}']\")\n",
    "        level_select.click()\n",
    "        \n",
    "        unavailable = []\n",
    "        print(f\"Downloading {valid_levels[level]} Level TAPR Data for {year}...\")\n",
    "        \n",
    "        for var in variables:\n",
    "            print(f\"Checking for {file_prefix}{var} data...\")\n",
    "            # Updated file patterns to include level prefix and year\n",
    "            file_patterns = [\n",
    "                f\"{file_prefix}{var}_{year}.csv\",\n",
    "                f\"{file_prefix}{var}_{year}.dat\",\n",
    "                f\"{level}{var}_{year}.dat\",  # Some files might not include the prefix\n",
    "                f\"{level}{var}_{year}.csv\"\n",
    "            ]\n",
    "            \n",
    "            if any(os.path.isfile(os.path.join(full_dir_path, file)) for file in file_patterns):\n",
    "                print(f\"{var}_{year} already exists\")\n",
    "                unavailable.append(var)\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                select_data = driver.find_element(By.XPATH, f\"//input[@type='radio' and @name='setpick' and @value='{var}']\")\n",
    "                select_data.click()\n",
    "                \n",
    "                # Add a small delay after clicking the radio button\n",
    "                time.sleep(1)\n",
    "                \n",
    "                download = driver.find_element(By.XPATH, \"//input[@type='submit' and @value='Continue']\")\n",
    "                download.click()\n",
    "                print(f\"Downloaded {level if var == 'REF' else file_prefix}{var} for {year}\")\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                print(f\"{var} not found for {year}\")\n",
    "                unavailable.append(var)\n",
    "                continue\n",
    "        \n",
    "        available_vars = set(variables) - set(unavailable)\n",
    "        # do not shut down driver until time-out occurs or all available files have finished downloading\n",
    "        if wait_for_downloads(variables = available_vars, year = year, directory = full_dir_path):\n",
    "            for a_var in available_vars:\n",
    "                file_renamer(directory = full_dir_path, year = year, prefix = file_prefix, var = a_var, level = level)  \n",
    "        driver.quit()\n",
    "\n",
    "        ### DISTRICT TYPE DATA DOWNLOAD ###\n",
    "        if level == \"D\" and dist_type:\n",
    "            print(f\"Downloading District Type Data for {year}...\")\n",
    "            \n",
    "            if os.path.isfile(os.path.join(full_dir_path, f\"district_type{year}.csv\")):\n",
    "                print(f\"District Type Data for {year} already exists\") # don't download if it already exists\n",
    "                print(\"\")\n",
    "                continue\n",
    "\n",
    "            df = district_type_scraper(year) # get the dataframe with the sheet data\n",
    "            df.to_csv(f\"{dir_name}/district_type{year}.csv\") # save it to the raw_data{year} folder\n",
    "\n",
    "            print(f\"Downloaded District Type Data for {year}\")\n",
    "            print(\"\")\n",
    "\n",
    "        #Last step: Checking if files are .dat and converting to .csv\n",
    "        convert_dat_to_csv(full_dir_path)\n",
    "\n",
    "    print(\"All Data Downloaded!\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading District Level TAPR Data for 2018...\n",
      "Checking for DISTPROF data...\n",
      "PROF_2018 already exists\n",
      "Checking for DISTPERF1 data...\n",
      "PERF1 not found for 2018\n",
      "Checking for DISTGRAD data...\n",
      "GRAD_2018 already exists\n",
      "Checking for DISTSTAAR1 data...\n",
      "STAAR1_2018 already exists\n",
      "Checking for DISTREF data...\n",
      "REF_2018 already exists\n",
      "Checking for DISTPERF data...\n",
      "PERF_2018 already exists\n",
      "All downloads for 2018 completed successfully.\n",
      "\n",
      "Downloading District Type Data for 2018...\n",
      "District Type Data for 2018 already exists\n",
      "\n",
      "Downloading District Level TAPR Data for 2019...\n",
      "Checking for DISTPROF data...\n",
      "PROF_2019 already exists\n",
      "Checking for DISTPERF1 data...\n",
      "PERF1 not found for 2019\n",
      "Checking for DISTGRAD data...\n",
      "GRAD_2019 already exists\n",
      "Checking for DISTSTAAR1 data...\n",
      "STAAR1_2019 already exists\n",
      "Checking for DISTREF data...\n",
      "REF_2019 already exists\n",
      "Checking for DISTPERF data...\n",
      "PERF_2019 already exists\n",
      "All downloads for 2019 completed successfully.\n",
      "\n",
      "Downloading District Type Data for 2019...\n",
      "District Type Data for 2019 already exists\n",
      "\n",
      "Downloading District Level TAPR Data for 2020...\n",
      "Checking for DISTPROF data...\n",
      "PROF_2020 already exists\n",
      "Checking for DISTPERF1 data...\n",
      "PERF1 not found for 2020\n",
      "Checking for DISTGRAD data...\n",
      "GRAD_2020 already exists\n",
      "Checking for DISTSTAAR1 data...\n",
      "STAAR1_2020 already exists\n",
      "Checking for DISTREF data...\n",
      "REF_2020 already exists\n",
      "Checking for DISTPERF data...\n",
      "PERF_2020 already exists\n",
      "All downloads for 2020 completed successfully.\n",
      "\n",
      "Downloading District Type Data for 2020...\n",
      "District Type Data for 2020 already exists\n",
      "\n",
      "Downloading District Level TAPR Data for 2021...\n",
      "Checking for DISTPROF data...\n",
      "PROF_2021 already exists\n",
      "Checking for DISTPERF1 data...\n",
      "PERF1_2021 already exists\n",
      "Checking for DISTGRAD data...\n",
      "GRAD_2021 already exists\n",
      "Checking for DISTSTAAR1 data...\n",
      "STAAR1_2021 already exists\n",
      "Checking for DISTREF data...\n",
      "REF_2021 already exists\n",
      "Checking for DISTPERF data...\n",
      "PERF not found for 2021\n",
      "All downloads for 2021 completed successfully.\n",
      "\n",
      "Downloading District Type Data for 2021...\n",
      "District Type Data for 2021 already exists\n",
      "\n",
      "Downloading District Level TAPR Data for 2022...\n",
      "Checking for DISTPROF data...\n",
      "PROF_2022 already exists\n",
      "Checking for DISTPERF1 data...\n",
      "PERF1_2022 already exists\n",
      "Checking for DISTGRAD data...\n",
      "GRAD_2022 already exists\n",
      "Checking for DISTSTAAR1 data...\n",
      "STAAR1_2022 already exists\n",
      "Checking for DISTREF data...\n",
      "REF_2022 already exists\n",
      "Checking for DISTPERF data...\n",
      "PERF not found for 2022\n",
      "All downloads for 2022 completed successfully.\n",
      "\n",
      "Downloading District Type Data for 2022...\n",
      "District Type Data for 2022 already exists\n",
      "\n",
      "Downloading District Level TAPR Data for 2023...\n",
      "Checking for DISTPROF data...\n",
      "PROF_2023 already exists\n",
      "Checking for DISTPERF1 data...\n",
      "PERF1_2023 already exists\n",
      "Checking for DISTGRAD data...\n",
      "GRAD_2023 already exists\n",
      "Checking for DISTSTAAR1 data...\n",
      "STAAR1_2023 already exists\n",
      "Checking for DISTREF data...\n",
      "REF_2023 already exists\n",
      "Checking for DISTPERF data...\n",
      "PERF not found for 2023\n",
      "All downloads for 2023 completed successfully.\n",
      "\n",
      "Downloading District Type Data for 2023...\n",
      "District Type Data for 2023 already exists\n",
      "\n",
      "All Data Downloaded!\n"
     ]
    }
   ],
   "source": [
    "# Run the function with all currently available years and all the TAPR datasets\n",
    "tapr_2018_2023 = list(range(2018, 2024)) # all years with data that is currently available\n",
    "\n",
    "data_acronyms = ['PROF', 'PERF1', 'GRAD', 'STAAR1', 'REF', 'PERF'] # all the measures located on the TAPR website\n",
    "\n",
    "tea_scraper(years = tapr_2018_2023, variables = data_acronyms, level = \"D\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
